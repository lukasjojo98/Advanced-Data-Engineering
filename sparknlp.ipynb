{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-02T13:48:10.897256Z",
     "start_time": "2024-12-02T13:47:59.733602Z"
    }
   },
   "source": [
    "# Importieren der notwendigen Bibliotheken\n",
    "import sparknlp\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from sparknlp.base import DocumentAssembler\n",
    "from sparknlp.annotator import SentimentDLModel, Tokenizer, Normalizer, StopWordsCleaner\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Starten einer Spark-Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SentimentAnalysisExample\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Initialisieren von Spark NLP\n",
    "sparkNLP_version = sparknlp.version()\n",
    "print(f\"Spark NLP version: {sparkNLP_version}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version: 5.5.1\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:48:44.829798Z",
     "start_time": "2024-12-02T13:48:25.297079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Review-Daten in ein Pandas DataFrame einfÃ¼gen\n",
    "data = {\n",
    "    \"image\": [\"https://images-na.ssl-images-amazon.com/images/I/71eG75FTJJL._SY88.jpg\"],\n",
    "    \"overall\": [5.0],\n",
    "    \"vote\": [\"2\"],\n",
    "    \"verified\": [True],\n",
    "    \"reviewTime\": [\"01 1, 2018\"],\n",
    "    \"reviewerID\": [\"AUI6WTTT0QZYS\"],\n",
    "    \"asin\": [\"5120053084\"],\n",
    "    \"style\": [{\"Size:\": \"Large\", \"Color:\": \"Charcoal\"}],\n",
    "    \"reviewerName\": [\"Abbey\"],\n",
    "    \"reviewText\": [\"I now have 4 of the 5 available colors of this shirt... \"],\n",
    "    \"summary\": [\"Comfy, flattering, discreet--highly recommended!\"],\n",
    "    \"unixReviewTime\": [1514764800]\n",
    "}\n",
    "\n",
    "load_dotenv(\"./env.env\")\n",
    "path = os.getenv(\"DATA_PATH\")\n",
    "reviews_df = spark.read.json(path)\n",
    "valid_reviews_df = reviews_df.filter(reviews_df.overall.isNotNull())\n",
    "\n",
    "# Umwandeln des Pandas DataFrame in ein Spark DataFrame\n",
    "df = valid_reviews_df#spark.createDataFrame(pd.DataFrame(data))\n",
    "\n",
    "# Anzeigen der Struktur\n",
    "df.printSchema()\n"
   ],
   "id": "763a0a91536a51a0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- image: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- style: struct (nullable = true)\n",
      " |    |-- Color Name:: string (nullable = true)\n",
      " |    |-- Color:: string (nullable = true)\n",
      " |    |-- Configuration:: string (nullable = true)\n",
      " |    |-- Content:: string (nullable = true)\n",
      " |    |-- Denomination:: string (nullable = true)\n",
      " |    |-- Edition:: string (nullable = true)\n",
      " |    |-- Format:: string (nullable = true)\n",
      " |    |-- Item Package Quantity:: string (nullable = true)\n",
      " |    |-- Length:: string (nullable = true)\n",
      " |    |-- Offer Type:: string (nullable = true)\n",
      " |    |-- Package Quantity:: string (nullable = true)\n",
      " |    |-- Package Type:: string (nullable = true)\n",
      " |    |-- Pattern:: string (nullable = true)\n",
      " |    |-- Platform for Display:: string (nullable = true)\n",
      " |    |-- Platform:: string (nullable = true)\n",
      " |    |-- Size Name:: string (nullable = true)\n",
      " |    |-- Size:: string (nullable = true)\n",
      " |    |-- Style Name:: string (nullable = true)\n",
      " |    |-- Style:: string (nullable = true)\n",
      " |    |-- Subscription Length:: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      " |-- verified: boolean (nullable = true)\n",
      " |-- vote: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:48:55.455819Z",
     "start_time": "2024-12-02T13:48:54.914555Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Erstellen eines DocumentAssemblers (um Text in ein annotiertes Format zu bringen)\n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"reviewText\") \\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "# Tokenizer, Normalizer und Stopwords Cleaner\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"tokens\")\n",
    "\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"tokens\"]) \\\n",
    "    .setOutputCol(\"normalized\")\n",
    "\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "    .setInputCols([\"normalized\"]) \\\n",
    "    .setOutputCol(\"clean_tokens\")\n",
    "\n",
    "# Sentiment-Analyse-Modell\n",
    "sentiment_model = SentimentDLModel.pretrained(\"sentimentdl_use_twitter\", \"en\") \\\n",
    "    .setInputCols([\"clean_tokens\", \"document\"]) \\\n",
    "    .setOutputCol(\"sentiment\")\n",
    "\n",
    "# Aufbau der Pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    document_assembler,\n",
    "    tokenizer,\n",
    "    normalizer,\n",
    "    stopwords_cleaner,\n",
    "    sentiment_model\n",
    "])\n",
    "\n",
    "# Anpassen der Pipeline an die Daten\n",
    "model = pipeline.fit(df)\n",
    "\n",
    "# Vorhersagen auf den Datensatz anwenden\n",
    "result = model.transform(df)\n",
    "\n",
    "# Ausgabe der Sentiment-Ergebnisse\n",
    "result.select(\"reviewText\", \"sentiment.result\").show(truncate=False)\n"
   ],
   "id": "225cf5deac25ff27",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'JavaPackage' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Erstellen eines DocumentAssemblers (um Text in ein annotiertes Format zu bringen)\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m document_assembler \u001B[38;5;241m=\u001B[39m \u001B[43mDocumentAssembler\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \\\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39msetInputCol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreviewText\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;241m.\u001B[39msetOutputCol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdocument\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Tokenizer, Normalizer und Stopwords Cleaner\u001B[39;00m\n\u001B[0;32m      7\u001B[0m tokenizer \u001B[38;5;241m=\u001B[39m Tokenizer() \\\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;241m.\u001B[39msetInputCols([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdocument\u001B[39m\u001B[38;5;124m\"\u001B[39m]) \\\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;241m.\u001B[39msetOutputCol(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokens\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mG:\\PycharmProjects\\advanced-data-engineering\\.venv\\lib\\site-packages\\pyspark\\__init__.py:139\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    137\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m    138\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n\u001B[1;32m--> 139\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mG:\\PycharmProjects\\advanced-data-engineering\\.venv\\lib\\site-packages\\sparknlp\\base\\document_assembler.py:96\u001B[0m, in \u001B[0;36mDocumentAssembler.__init__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     94\u001B[0m \u001B[38;5;129m@keyword_only\u001B[39m\n\u001B[0;32m     95\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m---> 96\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mDocumentAssembler\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mclassname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcom.johnsnowlabs.nlp.DocumentAssembler\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     97\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_setDefault(outputCol\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdocument\u001B[39m\u001B[38;5;124m\"\u001B[39m, cleanupMode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdisabled\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32mG:\\PycharmProjects\\advanced-data-engineering\\.venv\\lib\\site-packages\\pyspark\\__init__.py:139\u001B[0m, in \u001B[0;36mkeyword_only.<locals>.wrapper\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    137\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMethod \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m forces keyword arguments.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m    138\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_kwargs \u001B[38;5;241m=\u001B[39m kwargs\n\u001B[1;32m--> 139\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mG:\\PycharmProjects\\advanced-data-engineering\\.venv\\lib\\site-packages\\sparknlp\\internal\\annotator_transformer.py:36\u001B[0m, in \u001B[0;36mAnnotatorTransformer.__init__\u001B[1;34m(self, classname)\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msetParams(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m_java_class_name \u001B[38;5;241m=\u001B[39m classname\n\u001B[1;32m---> 36\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_java_obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_new_java_obj\u001B[49m\u001B[43m(\u001B[49m\u001B[43mclassname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43muid\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mG:\\PycharmProjects\\advanced-data-engineering\\.venv\\lib\\site-packages\\pyspark\\ml\\wrapper.py:86\u001B[0m, in \u001B[0;36mJavaWrapper._new_java_obj\u001B[1;34m(java_class, *args)\u001B[0m\n\u001B[0;32m     84\u001B[0m     java_obj \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(java_obj, name)\n\u001B[0;32m     85\u001B[0m java_args \u001B[38;5;241m=\u001B[39m [_py2java(sc, arg) \u001B[38;5;28;01mfor\u001B[39;00m arg \u001B[38;5;129;01min\u001B[39;00m args]\n\u001B[1;32m---> 86\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mjava_obj\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mjava_args\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mTypeError\u001B[0m: 'JavaPackage' object is not callable"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Konvertieren der Spark DataFrame zu Pandas DataFrame\n",
    "result_df = result.select(\"reviewText\", \"sentiment.result\").toPandas()\n",
    "\n",
    "# Ausgabe der Pandas DataFrame\n",
    "print(result_df)\n"
   ],
   "id": "22e51bca2b2c5d1e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
